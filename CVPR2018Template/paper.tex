\documentclass{article}%
\usepackage{lastpage}%
\usepackage{amsmath}%
%
\usepackage{amsmath}%提供数学公式支持

\usepackage{graphics}%用于添加图片
\usepackage{graphicx}%加强插图命令
\newcommand{\figpath}[1]{contents/fig/#1}

\usepackage{fontspec}%用于配置字体
\usepackage[table]{xcolor}%用于各种颜色环境
\usepackage{enumitem}%用于定制list和enum
\usepackage{float}%用于控制Float环境，添加H参数（强制放在Here）
\usepackage[colorlinks,linkcolor=airforceblue,urlcolor=blue,anchorcolor=blue,citecolor=green]{hyperref}%用于超链接，另外添加该包目录会自动添加引用。

\usepackage[most]{tcolorbox}%用于添加各种边框支持
\usepackage[cache=true,outputdir=./out]{minted}%如果不保留临时文件就设置cache=false,如果输出设置了其他目录那么outputdir参数也有手动指定，否则会报错。
\tcbuselibrary{minted}%加载tcolorbox的代码风格

\usepackage[a4paper,left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}%用于控制版式
\usepackage{appendix}%用于控制附加文件
\usepackage{ifthen}

\usepackage{pdfpages}%用于支持插入其他pdf页
\usepackage{booktabs}%目前用于给表格添加 \toprule \midrule 等命令
\usepackage{marginnote} %用于边注
\usepackage[pagestyles,toctitles]{titlesec} %用于标题格式DIY
% \usepackage{fancyhdr}%用于排版页眉页脚
\usepackage{ragged2e} % 用于对齐
%\usepackage{fixltx2e} %用于文本环境的下标 % 2015 版本后已经不再需要了
\usepackage{ulem} %用于好看的下划线、波浪线等修饰
\usepackage{pifont} %数学符号
\usepackage{amssymb} %数学符号

%\usepackage{fontspec}
%\setmainfont{DejaVu Serif}


\definecolor{langback}{RGB}{245,244,250}
\definecolor{langbacktitle}{RGB}{235,233,245}
\definecolor{langtitle}{RGB}{177,177,177}
\definecolor{langno}{RGB}{202,202,202}
\tcbset{arc=1mm}
\renewcommand{\theFancyVerbLine}{\sffamily\textcolor{langno}{\scriptsize\oldstylenums{\arabic{FancyVerbLine}}}}%重定义行号的格式
\newtcblisting{langbox}[1][tex]{%参考自https://reishin.me/tmux/ 的代码框样式
    arc=1mm,breakable,
    colframe=langbacktitle,
    colbacktitle=langbacktitle,
    coltitle=langtitle,
    fonttitle=\bfseries\sffamily,
    lefttitle=1mm,toptitle=0.5mm,bottomtitle=0.5mm,
    title = Code,
    drop shadow,
    listing engine=minted,
    minted style=colorful,
    minted language=#1,
    minted options={fontsize=\small,breaklines,autogobble,linenos,numbersep=2mm,xleftmargin=1mm},
    colback=langback,listing only,
    bottomrule=0mm,leftrule=0mm,toprule=0mm,rightrule=0mm,
    enhanced,
    % overlay={\begin{tcbclipinterior}\fill[langback] (frame.south west)rectangle ([xshift=5mm]frame.north west);\end{tcbclipinterior}}
}

\definecolor{boxback}{RGB}{245,246,250}
\newtcolorbox{markquote}{
    colback=boxback,fonttitle=\sffamily\bfseries,arc=0pt,breakable,
    boxrule=0pt,bottomrule=-1pt,toprule=-1pt,leftrule=-1pt,rightrule=-1pt,
    drop shadow,enhanced
}

\usepackage[UTF8,heading=true]{ctex}
\ctexset{
	section = {
	number = 第\chinese{section}章,
	format = \zihao{3}\bfseries,
	},
	subsection = {
	number = \arabic{section}.\arabic{subsection},
	format = \Large\bfseries
	},
	subsubsection = {
	number = \arabic{section}.\arabic{subsection}.\arabic{subsubsection},
	format = \Large\bfseries,
	},
    paragraph = {
	format = \large\bfseries,
	},
    subparagraph = {
	format = \large\bfseries,
	},
}

\setlength{\parindent}{2em}%设置首行缩进
\linespread{1.3}%设置行距

\setlength{\parskip}{0.5em}%设置段间距
\setcounter{tocdepth}{4}%设置目录级数
\setcounter{secnumdepth}{3}


\newtcbox{\inlang}[1][red]{on line,
arc=0pt,outer arc=0pt,colback=#1!10!white,colframe=#1!50!black,
boxsep=0pt,left=1pt,right=1pt,top=2pt,bottom=2pt,
boxrule=0pt,bottomrule=-1pt,toprule=-1pt,leftrule=-1pt,rightrule=-1pt}

\newlength\tablewidth


\definecolor{tablelinegray}{RGB}{221,221,221}
\definecolor{tablerowgray}{RGB}{247,247,247}
\definecolor{tabletopgray}{RGB}{245,246,250}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}

%
%
\begin{document}%
\normalsize%
%
Handle This House Property Dialog Dataset Better%

%
\section{Abstract}%

%
Dialog matching task is to estimate the matching degree of an answer to a question. To extract features from texts, an encoder{-}decoder structure is aften used. In this paper, I introduced the methods I used to learn this task on a dialog dataset in house property area from the 2020 CCF contest. First, I designed different models to find a way to extract features from texts better. Then, I also tried some methods to alleviate the problem about long{-}tail distribution and class{-}imblance. Finally, my methods reached 0.7679 after fine{-}tuned with 5{-}fold cross validation.%

%

%
\section{Introduction}%

%
Dialog systems are becoming more and more important area in natural language processing. An important research topic in dialogue systems is dialog matching. As illustrated in Figure{[}{]}, given a pair of question and answer sentence, dialog matching aims to judge whether the anwser is precisely enough to this question. This is a hard task as there may be a huge semantic gap between questions and their answers.%

%
Recently, different works in different area has been proposed to handle this problem. {[}Jointly Optimizing Diversity and Relevance in Neural Response Generation{]} proposed a method by using latent space and map different types of answer in this latent space around context. {[}You Impress Me: Dialogue Generation via Mutual Persona Perception{]} define a new conception, mutual persona perception, to model the dialog context. Besides, some methods found that better results can be reached by using a pre{-}trained from unsupervised large text datasets and well{-}designed model to extract feature from text first. Like Transformer{[}BERT: Pre{-}training of Deep Bidirectional Transformers for Language Understanding{]},GPT{[}Improving Language Understanding by Generative Pre{-}Training{]}. Meanwhile, text augmentation is another direction, which helps people train robust model.%

%
In this paper, to get a better result on the dialog dataset in house property area from the 2020 CCF contest, I try different methods, including data{-}preprocessing, model designing, loss constructing, etc.. My key contributions of this work are:%

%
\begin{itemize}%
\item%
Propose a pipeline framework for handling dialog matching problem.%
\item%
I try different methods to reach a better results in each part.%
\item%
My experiment reaches 0.768 on test dataset of this contest.%
\end{itemize}%
%

%

%
\section{Related work}%

%
\subsection{Data Augmentation}%

%
/https://arxiv.org/pdf/1901.11196.pdf/%

%
Automatic data augmentation is commonly used in computer vision and speech. It's useful to help people train more robust models, especially when using smaller datasets. \textit{However, it's hard to come up with generalized rules to preprocess natural language.} Some previouse works have proposed some techniques for data augmentation in NLP. {[}Qanet: Combining local convolution with global self{-}attention for reading comprehension{]} generated new data by translating sentences into another language and back into the original one. Other work augmented data by adding noise{[}Data noising as smoothing in neural network language models.{]} and replace word by synonym{[}Contextual augmentation: Data augmentation by words with paradigmatic relations{]}. Due to these work is hard to use in practice because of the high cost of implementation problem, {[}EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks{]} presented a simple set of univsal data augmentatoin techniques, which can efficiently imporve the result, particularly for smaller datasets.%

%

%
\subsection{Pre{-}train model}%

%
/Improving Language Understanding by Generative Pre{-}training，https://cdn.openai.com/research{-}covers/language{-}unsupervised/language\_understanding\_paper.pdf/%

%
Unsupervised pre{-}training is a common method to find a good initialization instead of train a randomly initialized model. Previous works explored this technique in image classification, but recent years, some research demonstrated that pre{-}training also works in NLP. Like BERT{[}{]}, GPT{-}1, GPT{-}2. But use this model is also a trade{-}off between performence and memory/time cost.%

%
\subsection{Class{-}imblance}%

%
Most read{-}world datasets do not have exactly equal number of instances in each class. A small difference often does not matter, but when the data has a long{-}tail distribution, it aften hurt the model performence. This phenomenon is commonly seen in different area, such as Medical image processing, object segmentation, etc.. In dialog dataset, responses like 'yes' is the most common things, so there is also a class{-}imblance problem. To handle this problem, different methods have been proposed. {[}Focal Loss for Dense Object Detection{]} constructed a loss named Focal{-}loss to down{-}weights the loss assigned to well{-}classified examples. {[}Learning to Reweight Examples for Robust Deep Learning{]} proposed a meta{-}learning method, to generate weights for current batch samples by using a small clean class{-}blanced validation dataset. All these methods reached good results on their areas.%

%

%
\section{Method}%

%
Before presenting the model, I first provide the problem formulation. Suppose that we have a dialog dataset  $\{Q,A,C,Y\}$  for matching, I denotes  $Q=\{q_1,q_2,...,q_m\}$  as a question set,  $A=\{a_1,a_2,...,a_n\}$  as a answer set,  $C={c_1,c_2,...,c_n},c_i\in [1,m]$  to indicate which  $q_j$  is  $a_i$  answer for, which means  $\left( q_{c_i},a_i\right)$  can be a pair or a sample.  $Y={y_1,y_2,...,y_n},y_i\in\{0,1\}$  as a label set, to indicate whether  $a_i$  can be a good answer to  $q_{ci}$ . Then the dataset can be reformulated to  $D = \{\left(q_{ci},a_i,y_i\right)_i\}^N_i=1$ .%

%
Given a dataset  $D$ , the objective function of training neural networks can be represented as:%

%
\[%
\underset{\theta}{\operatorname{argmax}}  \sum_{i=1}^N L\left(y_i,M\left(F\left(q_i;\theta_q\right),F\left(a_i;\theta_a\right);\theta\right)\right)\\%
\]%
%

%
where  $F$  denotes the encoder, and  $M$  with parameter  $\theta$  is defined to project features from the concat of question and answer to 2{-}dimention vector, which can be used for classification.%

%

%
\subsection{Encoder}%

%
Denote the map function as Transformer is  $T(\cdot)$ , for each input  $x$  the output  $s=T(x)$ , where  $s\in R^{T,F}$ ,  $T$  is timestamp,and  $F$  is the dimention of the feature in each timestamp. Since  $T \times F$  is a high dimention vector, It's better to project it into a lower dimention space by another map function. In this paper, I tried three methods.%

%

%
\textbf{Simple MultiLayer Perceptron} Simpliest way to reduce the dimention is to add multilayer perceptron. First, I build 4 vectors with  $F$ {-}dimention, the first two vectors are the first and the last features alone the timestamp axis of  $s$ , the last two are generated from average pooling and max pooling operation separately. Then I concatate these four vectors and continue reducing its dimention by a dense layer.%

%
\textbf{Resnet blocks} I found that the first method does not work well, and one possible reason might be the loss of a lot of information when using pooling operation. To extract more useful information from feature  $s$ , I build a 1D resnet by replacing all 2D operations in original Resnet{[}{]} into 1D opeartions, which can be used to extract feature from text features.%

%

%
\textbf{An Ensemble} A better feature can be extracted by an ensemble of different extractors. In this version, I build a two{-}branch decoder, where one of these is still a 1D resnet structure, and another is a multilayer perceptron layer alone the timestamp axis. Feature from two branchs are also concatenated and then a dense layer is used.%

%

%
Finally, I concatenate the feature from question  $q'_i$  and answer  $a_i$ , and map it to 2{-}dimention vector by adding a linear project layer. Then a cross{-}entropy loss function can be constructed. I further explored Focal{-}loss{[}{]} to alleviate the affect caused by class{-}imblance.%

%

%
\section{Experiments and Results}%

%
\subsection{Dataset}%

%
In this paper, I evaluate our methods on a dialog dataset in house property area from the 2020 CCF contest. It consists nearly 6K questions and 20K answers with labels for train, and 14K questions and 53K answers without labels for test. Due to its class{-}imblance and less train data that test, it's a challenging task.%

%
When preprocess the data, the sentence need to be converted into token id, and be padded or truncated to the same length. The length, which is a hyperparamter in our experiments, is also a trade{-}off. Larger length may improve the performance, and smaller can save more time. Finally, I set this value to 25.%

%

%
\subsection{Implementation Details and Results}%

%
First, I train my model using SGD with momentum of 0.9, a weight decay of {[}todo{]}, and a batch size of {[}todo{]}. The network is trained for 20 epochs. Test results are given by a 5{-}fold cross validation, which is a procedure used to estimate the skill of the model on new data. I find that most hyperparameters do not need to be heavily tuned. I first froze the parameter in BERT and treat it as a pure feature extractor to save memory, which reached a worse result, 0.69. Then I choice to train the whole model on a full TITAN XP, and I finally reached 0.7679.%

%

%
\section{Conclusion}%

%
In this paper, I mainly explored three methods for extracting useful method and reducing dimention from high{-}dimention vectors at the same time. I then mixed focal{-}loss and meta{-}learning method to try to handle the class{-}imblance problem. The results show that well{-}designed encoder can utilize the feature better that baseline, which only use one linear project layer. In the future, it would be interesting to further investigate better method to extract more useful information.%

%
\section{Reference}%

%

%
\end{document}